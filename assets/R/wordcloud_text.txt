Design of Experiments is key to efficient and economic knowledge discovery in many areas of science, engineering and medicine, as it facilitates the collection of data that provide as much information as possible on the aims of the experiment. Most commonly, these are some or all of model selection, estimation of unknown model parameters and accurate prediction of a response or a response feature. The scientific design of an experiment can lead to a substantial improvement in the accuracy and validity of the conclusions drawn from use of a given amount of resource.
There is a long history of synergy between methodological research in design and major application areas in academia and industry, with real problems providing motivation for the development of new methodology and a testbed for the assessment of new techniques. During the last five years, the subject has been reinvigorated, with research underway on areas such as nanotechnology [33], manufacturing [32], biotechnology [31] and virtual experimentation on computers [30]. There has also been an influx of PhD students and early career researchers into the field, creating the ideal opportunity to develop a new generation of international leaders.
It is in this context that my proposed research programme should be viewed. I plan to develop new statistical methodology to tackle current and emerging complex multi-factor experiments where the resulting data cannot be adequately modelled by a standard linear or simple nonlinear model. In particular, collaboration with academic scientists, applied mathematicians, and the two project partners, GlaxoSmithKline (GSK) and Dstl, have motivated my interest in the following three challenges:
1. Experiments on systems where the underlying response variable is not sufficiently simple to be adequately described by a given parametric form, such as a low-order polynomial model, or where it is unrealistic to assume such a form prior to performing the experiment. Examples from chemistry include experiments on flow systems [29] operated over a wide range of reaction conditions, when the reaction kinetics are not well understood, and experiments to develop formulations of chemical additives [28].
2. Experiments that collect multiple data points on each experimental unit. Often, data from each unit can be assumed to have been generated by a smooth function, representing a curve or surface. In recent years, the number of experiments that produce such functional data has greatly increased due to the widespread use of automated data measurement systems. Examples include the measurement of friction via a Stribeck curve in tribology [27], and computational dispersion models in atmospheric science [26].
3. Experiments where detailed scientific knowledge is available on the phenomenon to be investigated. Mechanistic (“first-principal”) and phenomenological models are increasingly used to explain experimental data through scientific theory. Mechanistic models may also provide a basis for “scale-up” from lab-based experiments to full-scale manufacturing. Experiments are required to collect data to inform a choice made between competing models (scientific explanations), to estimate unknown parameters in a particular model (calibration), and for model validation. A complication that often occurs is that the mechanistic model may not be an adequate description of the phenomenon. Then prediction can be improved by empirical modelling of the discrepancy between the mechanistic model and reality. Examples arise in experiments on dispersion models, and in the study of reaction kinetics [25] and other aspects of chemical processes such as distillation [24], which are important in maximising the yield of a new drug.
To address these challenges we need tailored designs of sufficient size to allow the estimation of realistic statistical models which incorporate prior knowledge of the scientific phenomenon and any hierarchical unit structures (eg split plots) imposed by the experiment procedures. Importantly, these models must have enough flexibility to adapt to the experimental data. There is a wealth of well-developed research on the modelling problem, for both empirical [59,60] and mechanistic [61] models. However, there is a dearth of designs for estimating these models, particularly for the multi-factor problems widely encountered in modern scientific and industrial applications. This is the methodological gap that my research programme will fill.

The research is timely in that it addresses the increasingly complicated and expensive design of experi- ment problems now arising in science and industry. There is a pressing need to develop new design methods tailored to the recent advances in statistical and mathematical modelling which can increase the the scientific understanding gained from these experiments. There have been a number of recent programmes and work- shops in areas related to this proposal, for example at the Isaac Newton Institute in Design of Experiments,
3
and at SAMSI in Computer Experiments and Uncertainty Quantification. The timeliness for the non-academic community is demonstrated by the support of two external project partners, each of whom will be actively involved in two research themes.
The award of a Fellowship will allow me to progress this research in a timely manner, unencumbered by teaching and administrative duties. During the past five years, I have gained considerable experience of interacting with industry and academic scientists. The Fellowship would allow me time to work on generic research motivated by their challenging scientific problems, and develop the necessary fundamental method- ology. I am uniquely placed in the UK to conduct this research, having the appropriate statistical background and expertise, and the scientific and industrial partners.

The methodologies developed will benefit researchers in Statistics by opening up new areas of research and by developing two new highly trained researchers through associated PhD projects. It will benefit academics in other fields through providing economical designs tailored to experiment objectives and modern data modelling techniques, especially in chemistry, tribology and dispersion science. My links with nCATS and ORC will enable wider dissemination to researchers in engineering science and photonics through these centres’ extensive national and international networks.
Research outputs will be disseminated to academic statisticians throughout the programme by conference presentations, publicly available technical reports (on arXiv) and papers in international journals for each challenge. I also plan to organise a session at a major international conference, targeting either the Joint Statistical Meetings or the next “Designed Experiments: Recent Advances in Methods and Applications (DEMA)” meeting. Presentations and papers relevant to applications will be prepared with collaborators and project partners, who have a wide range of academic links in their fields, and submitted to appropriate subject matter journals. To ensure accessibility of the methods, well documented R packages will be produced and made available through CRAN. A research blog and Wiki will provide rapid and wide access to results and a research repository.
Approaches to the third challenge in Section 2 have been discussed with Applied Mathematicians at Southampton (Please and MacArthur) through BtG activities and directions for collaborative work have been identified. The research has the potential to bring substantial benefits to applied mathematics modellers in their research on, for example, models as solutions to systems of complex partial differential equations, as it will enable more effective selection from competing models, stimulating ideas on the underlying science, and more accurate estimation of unknown parameters with associated uncertainties.
Aspects of the research will be carried out in collaboration with Professor Hugh Chipman (Canada Research Chair, Acadia) and Dr Peter Qian (Wisconsin-Madison). Chipman is an expert in nonparametric regression and Bayesian computation; Qian is an expert in computer experiments and hierarchical space-filling designs. Both Chipman and Qian have hosted short-term visits for me in the recent past, and have agreed to collaborate in this programme.
A further collaboration will be with researchers at the Queensland University of Technology through a recently announced Australian Research Council Discovery Grant (PI Pettitt) on which I am a Co-I. This project, which aims to find designs for different challenges to those outlined above, provides funds for an extended visit by me to Australia and is complementary to the programme in my fellowship proposal.

The objectives for this proposal are to
a. develop novel design methodology for efficient and accurate model discrimination and variable selection, parameter estimation and prediction from complex experiments arising from modern technologies;
b. demonstrate, evaluate and iteratively improve the new methods including through interaction with the project partners and other collaborators, and through application to their experimental programmes;
c. implement the methodology in computationally efficient algorithms and make them widely available.
These objectives will be progressed through three inter-related research themes, described below, which directly address the challenges in Section 2 and which concern the selection of a finite set of design points,
4
not necessarily distinct, from a design region X. An observation yj is made at each design point xj = (x1j , . . . , xmj ), where xij is the value taken by the ith factor at the jth design point, or run, in the experiment (j = 1, . . . , n). The relationship between yj and xj is described by
yj =g(xj)+error, j=1,...,n, (1)
where g represents the m-dimensional surface about which we wish to learn. The joint distribution of the errors depends on the experiment structure, for example, through restrictions on the randomisation of the design points to units. The themes differ in the nature of the response being observed, and the assumptions made on g.
Theme 1. Designs for multi-factor nonparametric smoothing and regression: use of a nonparametric estimator, gˆ(x), provides a data-driven and flexible fitted model [39] which makes fewer a priori assumptions about the complexity of g. Building on existing literature for local linear smoothers [37,58] and my ongoing work with a PhD student on single factor studies, I aim to develop the first general methodology for multi-factor experiments for a variety of nonparametric methods. While each model will clearly present individual design challenges, common issues and methodology will be identified with the aim of developing a paradigm for multi-factor nonparametric design.
Theme 2. Designs for functional data: Suppose that model (1) is extended such that, for the jth run, we obtain s data values, (tlj,ylj), from a smooth function y(t; xj) (l = 1,...,s) [34]. The aim of an experiment producing such data might be to predict the function y(t; x) at unobserved x or to compare functions corresponding to different design points (cf factorial comparisons). Novel design methods will be developed for (i) y(t; x) having a known parametric form, and (ii) y(t; x) being estimated nonparametrically as a further development of work under theme 1.
Theme 3. Designs for selection and improvement of mechanistic models: Suppose that there is theoretical knowledge related to the physical process that leads to the formulation of a set of contending mechanistic models, labelled 1, . . . , q. Suppose also that the unknown function g(·) has the form
g(xi) = fk(xi; θk) + dk(xi) , (2)
where fk is the kth mechanistic model, θk holds the unknown parameters of the kth model, and dk is a function describing the discrepancy between fk and g. The aim here is to find efficient designs for discrimi- nating between these models, for estimating the parameters in a chosen model, and for making predictions. It should be recognised that none of the mechanistic models may provide a completely adequate description of the response. Rather than considering the special case of dk ≡ 0 for all k (no discrepancy), for which there is a substantial design literature [17, 22, 57], I will investigate models for which dk ̸≡ 0, which has been studied in the modelling literature [50,51] but for which no tailored designs currently exist.
Achieving objectives a-c for each of these themes will meet the research challenges outlined in Section 2. Within each theme, there is deliberate scope for new research directions and adventure to emerge during the course of the programme.


The following plan identifies the main research activities within the three themes. The programme assumes that the majority of the research will be undertaken by myself, as lead researcher, with contributions from PhD students funded from other sources and in collaboration with Chipman (within theme 1) and Qian (within theme 3).
A fundamental problem for all three design themes in Section 4 is the variety of uncertainties present when planning an experiment. Failing to account for uncertainties can seriously reduce a design’s capacity to meet the aims of the experiment. These uncertainties include the following:
i. The unknown values of the model parameters which, for most of the statistical models to be considered, can have a strong influence on design performance, eg because the model is a nonlinear function of the parameters.
ii. The unknown functional form of the final model fitted to the data when it is planned to estimate the underlying surface using nonparametric methods, or when there are competing mechanistic models.
5
iii. The form of the error structure in (1), due to hierarchies in the experimental units or measurement systems; for example, due to nested functional data or other restrictions in the randomisation. When data are collected over time or space, serial correlations are also likely be present.
In view of these uncertainties, a Bayesian approach [23] is natural and will be adopted for this programme. Design of experiments is an “a priori” activity, taking place before data has been collected, and hence the Bayesian paradigm is particularly appropriate. It allows available prior information on the model to be incor- porated into both the design and analysis, and produces posterior distributions that are more interpretable by scientists. It also reduces reliance on unrealistic assumptions and asymptotic results that may be inappropriate for small to medium-sized experiments. To avoid these drawbacks, I will make use of simulation from exact posterior distributions using Markov Chain Monte Carlo or, where appropriate, approximate results via analytic substitutions.
The research themes will be progressed through methodological research from each of the following common underpinning topics.
Utility functions and Bayesian design: Central to Bayesian decision theoretic design [23,55] is the definition of a utility function that encapsulates mathematically the aims of the experiment. Suppose that u(ξ,y,θ,φ) is the utility for the Bayes decision for design ξ under data y, with parameters θ and model φ. Then an optimal design ξ⋆ is such that the expected utility is maximised, i.e.
ξ⋆ =argmaxξ
􏰁􏰁􏰁
yφθ
u(ξ,y,θ,φ)p(θ,φ,y|ξ)dθdφdy, (3)
where θ ∈ Θ, the parameter space, and φ ∈ Φ, the model space. If Φ is discrete, the integral with respect to φ is replaced by a summation. When the aim of the experiment is prediction, as is primarily the case in themes 1 and 2, we define up(ξ,y ̃,y) as the utility for the prediction of y ̃ and calculate the utility function in (3) as u(ξ, y, θ, φ) = 􏰀 up(ξ, y ̃, y)p(y ̃|θ, φ, y) dy ̃.
y ̃
There are a number of difficulties in calculating the expected utility in (3): (a) the evaluation of u itself
is potentially non-trivial, as it depends on the Bayes decision which may only be available numerically; (b) the calculation of the joint density p(θ,φ,y|ξ) = p(y|θ,φ,ξ)p(θ|φ)p(φ) may be complicated by the nonparametric model assumptions in theme 1, and by the expense of calculating the likelihood for some numerical mechanistic models in theme 3; (c) the integrals in (3) may be very high dimensional, and are unlikely to be analytically tractable; and (d) the joint probability model must take account of hierarchies in the experiment structure (especially in theme 2).
To overcome these problems, I will investigate efficient MCMC schemes and the alternative analytic approx- imations of variational Bayes and integrated nested Laplace [20,21] for the calculation of u. To mitigate the risk of the calculation of (3) being computationally infeasible, I will develop, where appropriate, utility functions that do not depend on y and then evaluate the potential for design (rather than analysis) of approximating p(θ|y) by p(θ). This pseudo-Bayesian approach has proved effective in nonlinear optimal design [10]. There are also tailored quadrature schemes [47] available for the calculation of the expected utility in this case which I will investigate and extend as necessary.
Sequential design: As an alternative to selecting all the design points prior to the start of data col- lection, a sequential design strategy [45], where practicable, offers greater efficiency when design performance depends on unknown variables and parameter values. Sequential design fits naturally within a Bayesian ap- proach: as the sequence of design points is developed, progressively more informative distributions become available to obtain the expected utility, defined as an extension of (3). My research will consider both traditional point sequential designs that select and run one extra design point at a time, and batch sequential designs in which a set of more than one new point is selected at each step. In many scientific settings, experimental runs are performed in parallel (eg using robotic systems) and hence there is a need for batch sequential designs. I will explore approaches to incorporating the additional hierarchies (blocks) that are required to account for possible variation in experimental conditions from batch to batch, and investigate how to handle the greater complexity in the calculation of the utility.
A further interesting research topic is non-myopic sequential design [46,53], where the selection of the next design point, or batch of points, takes account of the observations that will be made in the future. In a Bayesian context, this means that design selection at step v in a point sequential strategy is based on a function of the posterior distributions incorporating up to a further w points, p(θ|y1,...,yv−1), p(θ|y1,...,yv), ...,
6

p(θ|y1, . . . , yv+w−1). The “horizon” w may be defined, for example, by the resource available for the whole experiment; typically, the posterior density for θ is given less weight as we get closer to the horizon. Clearly, we need to take account of uncertainty not only in future y values but also in future values of the chosen design points x. I will explore this issue through formulation of an updated probability density defined on the design region and based on the utility function. I will investigate the extent to which this and other possible ways of taking account of future design points, in addition to the use of previously collected data, can lead to more effective experiments when planning the next step in a sequential strategy.
Computational algorithms for design selection: For many problems from science and industry, computational approaches will be needed to evaluate the utility function and perform the subsequent opti- misation necessary to find an optimal design. For point sequential design, I will explore the use of particle filters and other Sequential Monte Carlo methods (SMC [43]), which offer considerable possibilities for more computationally efficient design selection [42,44]. The application of SMC to design problems is not a mature area, and their potential use for batch sequential problems is unexplored.
To find maxima of the expected utility, I will investigate the effectiveness and computational feasibility of a variety of optimisation algorithms for finding each of non-sequential designs and batch sequential designs and as an alternative to using SMC for finding point sequential designs. The most successful algorithm for finding multi-factor designs to date has been the coordinate exchange [41]. Novel tailored variants of this algorithm will be developed to find designs within each of the three research themes. Evolutionary algorithms and other nature-inspired heuristics, such as genetic algorithms, ant colony and swarm algorithms [40], will also be investigated and necessary developments made to enable their application to solve the design problems.
Research in these topics under the three themes described in Section 4 will proceed as follows, with the stated milestones. Each milestone has the potential to result in a methodological paper. However, it is likely that some of the results will be combined into a single paper, eg M1 and M2.
Theme 1: Initial research will focus on both non-sequential and sequential designs for generalised additive models (GAMs [38]) which provide a convenient framework for variable selection. I have already made some progress in this area by establishing optimal non-sequential designs through analytic and numerical methods for partially linear additive models [2]. I will build on this work to find optimal and highly efficient designs for a wider class of GAMs (Milestone M1).
Later research, in collaboration with Chipman, will address design problems from other multi-factor models, such as Gaussian process models, thin-plate splines, and classification and regression trees. Gaussian processes [18], in particular, are widely used models and with a growing literature on associated design methods [35,36,54]. These designs will be used as comparators in the evaluation of the new design methods. As the minimal assumptions that can be made for nonparametric regression prior to data collection may negate the usual efficiency gains obtained from model-based design, another important comparator will be space-filling designs [56] which make no model assumptions. (M2).
When finding non-sequential designs, I will make various different assumptions on the true function g in model (1) to calculate p(y|ξ), and hence it will be necessary to assess the impact of these assumptions on the properties of the designs produced. For example, I will consider making an assumption that g is bounded [48] or that g can be adequately described by a suitable complex smoother with known parameters. The robustness of design performance, especially efficiency, to these assumptions will also be assessed. More adventurously, in later research I will consider design for full Bayesian nonparametrics, for example, Polya trees and Dirichlet processes [49].
Theme 2: I will begin by finding optimal and highly efficient designs (both sequential and non-sequential) for the extension of model (1) and the common and more straightforward case (Case I) of fixed tj = (t1j , . . . , tsj ) and large s (M3). Methodology will then be developed for the more general, hierarchical, design problem (Case II) of the joint selection of xj and tj (M4). This hierarchy will induce a more complex structure for the error term, including both within and between run variation and, potentially, serial correlation within run.
In addition, I will find designs for experiments in which certain input factors are controllable functions of t [52] but where both xj and tj can be chosen (M5). In such experiments, the selection of a functional form for these factors is part of the design process and should reflect the aims of the experiment.
Theme 3: Work on this theme will begin by finding Bayesian and sequential designs when the discrepancy dk in model (2) has known parametric form (M6). Then, methodology will be developed for the more realistic
7

situation when dk is unknown and to be estimated using nonparametric methods (M7). Batch sequential design will introduce hierarchies into the unit structure that must be incorporated into the design. Interaction with Qian [15,16] will enable further development of hierarchical space-filling designs as comparators for the new model-based methodology, as well as providing an opportunity for new ideas to emerge.
It is anticipated that for some models, fk may be too computationally expensive for the likelihood p(y|θ,φ,ξ) to be calculated routinely. This problem will be overcome by development of a fast emulator as a surrogate for the model (cf computer experiments and [19]).
In some application areas, the mechanistic model may depend only on a subset of the m factors in the experiment. By modelling the discrepancy dk(x) in terms of all the factors, including those not present in the mechanistic model, greater scientific understanding may be achieved and guidance provided for applied mathematicians or subject experts on iterative improvement of the mechanistic model. A particular focus of the research in this theme will be the development of new modelling and design strategies for this situation, and their critical assessment (M8), in collaboration with applied mathematicians (Please).
Practical application: Central to the development and evaluation of the new methodology will be substantive applications provided by the project partners. These will act as a motivating framework, enable the development of realistic simulation scenarios to assess the methods, and provide a testbed for practical evaluation and feedback through real experimentation in the latter half of the programme. Potentially suitable applications have already been identified with the project partners and collaborators. These are: for theme 1, reaction optimisation in flow chemistry (GSK); for theme 2, describing and predicting Stribeck curves (nCATS); and for theme 3, understanding numerical dispersion models (Dstl) and possibly reaction kinetics (GSK). My growing collaboration with ORC and Applied Mathematics is related to themes 2 and 3 and has recently been supported by the University of Southampton with £50K to carry out proof-of-principle multi-factor investigations.
Both project partners have agreed to perform at least one experiment or, in the case of Dstl, provide computer model runs and available field data. Further, nCATS have also agreed to run an experiment and there is an expectation that further experiments will be run by ORC. Prior to running any real experiments, historical datasets, where available, will be used to inform virtual experiments for assessment of the methods.
Project team and management: I will manage the research programme, joint work with the project partners and collaborators, and the dissemination and impact activities to both academic and non-academic communities. Two PhD students will contribute to the research under my supervision (with at least weekly meetings). The first is working within theme 2 (wef October 2011) and the second within theme 2 (from Year 3); both are supported by the University of Southampton, and will be trained through APTS and the University Graduate School. I plan to expand and sustain the research team through at least one multi-disciplinary grant proposal, which will fund a Statistics PDRA and enable progression of the research beyond that outlined in this proposal. The applied statisticians and scientists from the project partners will also contribute to the work of the team, especially the planning and interpretation of the experiments.
Collaboration with Chipman (on theme 1) and Qian (theme 3) will be achieved through one month visits to Acadia (around the end of Year 1) and Madison (around the end of Year 3). Regular interaction will be conducted electronically through email and video conferencing. In addition, informal meetings will take place at conferences such as the Joint Statistical Meetings.
Each of the project partners has committed substantial staff time and resource. Collaboration with, and feedback from, these partners will be managed through once yearly meetings at each of GSK and Dstl with teams of lead statisticians and scientists. These meetings will provide a forum to discuss research progress and goals, and obtain feedback and new ideas. Outside these meetings, regular contact will again be maintained electronically, with additional face-to-face meetings as necessary.
The attached diagrammatic workplan gives indicative timelines for the research activities.

